<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=63665&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>RAG Notes | An eclectic mix of technical content!</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="What Problems Does LlamaIndex Solve? A way to ingest data into a vector store A way to create queries Notes about LlamaIndex Note that Behind the Scenes:
the input documents are loaded
the input documents are chunked, embeddings created and stored in a vector db
The vector db is created a query engine combines:
Retriever turns the question into an embedding that can be sued to call the vector db Calls the vector db to get the best matches Performs postprocessing ion retreived nodes Synthesizes the query to a LLM">
    <meta name="generator" content="Hugo 0.123.7">
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    

    
      

    

    

    
      <link rel="canonical" href="http://localhost:63665/posts/ai/rag/rag-notes/">
    

    <meta property="og:title" content="RAG Notes" />
<meta property="og:description" content="What Problems Does LlamaIndex Solve? A way to ingest data into a vector store A way to create queries Notes about LlamaIndex Note that Behind the Scenes:
the input documents are loaded
the input documents are chunked, embeddings created and stored in a vector db
The vector db is created a query engine combines:
Retriever turns the question into an embedding that can be sued to call the vector db Calls the vector db to get the best matches Performs postprocessing ion retreived nodes Synthesizes the query to a LLM" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:63665/posts/ai/rag/rag-notes/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-04-03T08:50:13-05:00" />
<meta property="article:modified_time" content="2024-04-03T08:50:13-05:00" />

<meta itemprop="name" content="RAG Notes">
<meta itemprop="description" content="What Problems Does LlamaIndex Solve? A way to ingest data into a vector store A way to create queries Notes about LlamaIndex Note that Behind the Scenes:
the input documents are loaded
the input documents are chunked, embeddings created and stored in a vector db
The vector db is created a query engine combines:
Retriever turns the question into an embedding that can be sued to call the vector db Calls the vector db to get the best matches Performs postprocessing ion retreived nodes Synthesizes the query to a LLM"><meta itemprop="datePublished" content="2024-04-03T08:50:13-05:00" />
<meta itemprop="dateModified" content="2024-04-03T08:50:13-05:00" />
<meta itemprop="wordCount" content="344">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="RAG Notes"/>
<meta name="twitter:description" content="What Problems Does LlamaIndex Solve? A way to ingest data into a vector store A way to create queries Notes about LlamaIndex Note that Behind the Scenes:
the input documents are loaded
the input documents are chunked, embeddings created and stored in a vector db
The vector db is created a query engine combines:
Retriever turns the question into an embedding that can be sued to call the vector db Calls the vector db to get the best matches Performs postprocessing ion retreived nodes Synthesizes the query to a LLM"/>

	
  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        An eclectic mix of technical content!
      
    </a>
    <div class="flex-l items-center">
      

      
      
<div class="ananke-socials">
  
</div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked ttu">
          
        Posts
      </aside>
      










  <div id="sharing" class="mt3 ananke-socials">
    
  </div>


      <h1 class="f1 athelas mt3 mb1">RAG Notes</h1>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2024-04-03T08:50:13-05:00">April 3, 2024</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><h2 id="what-problems-does-llamaindex-solve">What Problems Does LlamaIndex Solve?</h2>
<ul>
<li>A way to ingest data into a vector store</li>
<li>A way to create queries</li>
</ul>
<h2 id="notes-about-llamaindex">Notes about LlamaIndex</h2>
<p>Note that Behind the Scenes:</p>
<ul>
<li>
<p>the input documents are loaded</p>
</li>
<li>
<p>the input documents are chunked, embeddings created and stored in a vector db</p>
<ul>
<li>The vector db is created</li>
</ul>
</li>
<li>
<p>a query engine combines:</p>
<ul>
<li>Retriever
<ul>
<li>turns the question into an embedding that can be sued to call the vector db</li>
<li>Calls the vector db to get the best matches</li>
</ul>
</li>
<li>Performs postprocessing ion retreived nodes</li>
</ul>
</li>
<li>
<p>Synthesizes the query to a LLM</p>
<ul>
<li>Creates a context
<ul>
<li>Combines the question, context and prompt</li>
<li>Submits the prompt to the LLM</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Things you can cusotmize:</p>
<ul>
<li>How the documents are processed (split)</li>
<li>Embedding used (model, vector size, )</li>
<li>Upsert to VDB</li>
<li>Query parameters</li>
<li>Prompt</li>
<li>LLM Used</li>
</ul>
</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>const documents <span style="color:#f92672">=</span> <span style="color:#66d9ef">await</span> new SimpleDirectoryReader()
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">.</span>loadData({directoryPath: <span style="color:#e6db74">&#34;./data&#34;</span>})
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>const index <span style="color:#f92672">=</span> <span style="color:#66d9ef">await</span> VectorStoreIndex<span style="color:#f92672">.</span>fromDocuments(documents)
</span></span><span style="display:flex;"><span>const queryEngine <span style="color:#f92672">=</span> index<span style="color:#f92672">.</span>asQueryEngine()
</span></span><span style="display:flex;"><span>const response <span style="color:#f92672">=</span> <span style="color:#66d9ef">await</span> queryEngine<span style="color:#f92672">.</span>query({
</span></span><span style="display:flex;"><span>    query: <span style="color:#e6db74">&#34;What did the author do in college?&#34;</span>
</span></span><span style="display:flex;"><span>})
</span></span><span style="display:flex;"><span>console<span style="color:#f92672">.</span>log(response<span style="color:#f92672">.</span>toString())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># In order to customize do this:</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>let customServiceContext <span style="color:#f92672">=</span> new llamaIndex<span style="color:#f92672">.</span>serviceContextFromDefaults({
</span></span><span style="display:flex;"><span>    llm: new llamaIndex<span style="color:#f92672">.</span>OpenAI(),
</span></span><span style="display:flex;"><span>    embedModel: new llamaIndex<span style="color:#f92672">.</span>OpenAIEmbedding()
</span></span><span style="display:flex;"><span>})
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>let customQaPrompt <span style="color:#f92672">=</span> function({context <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;&#34;</span>, query <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;&#34;</span>}) {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#960050;background-color:#1e0010">`</span>Context information <span style="color:#f92672">is</span> below<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">---------------------</span>
</span></span><span style="display:flex;"><span>        <span style="color:#960050;background-color:#1e0010">$</span>{context}
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">---------------------</span>
</span></span><span style="display:flex;"><span> 
</span></span><span style="display:flex;"><span>        Given the context information, answer the query<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span>        Include a random fact about whales <span style="color:#f92672">in</span> your answer<span style="color:#f92672">.</span>\
</span></span><span style="display:flex;"><span>        The whale fact can come <span style="color:#f92672">from</span> your training data<span style="color:#f92672">.</span>
</span></span><span style="display:flex;"><span> 
</span></span><span style="display:flex;"><span>        Query: <span style="color:#960050;background-color:#1e0010">$</span>{query}
</span></span><span style="display:flex;"><span>        Answer:<span style="color:#960050;background-color:#1e0010">`</span>
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>let customResponseBuilder <span style="color:#f92672">=</span> new llamaIndex<span style="color:#f92672">.</span>SimpleResponseBuilder(
</span></span><span style="display:flex;"><span>    customServiceContext,
</span></span><span style="display:flex;"><span>    customQaPrompt
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>let customSynthesizer <span style="color:#f92672">=</span> new llamaIndex<span style="color:#f92672">.</span>ResponseSynthesizer({
</span></span><span style="display:flex;"><span>    responseBuilder: customResponseBuilder,
</span></span><span style="display:flex;"><span>    serviceContext: customServiceContext
</span></span><span style="display:flex;"><span>})
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>let customRetriever <span style="color:#f92672">=</span> new llamaIndex<span style="color:#f92672">.</span>VectorIndexRetriever({
</span></span><span style="display:flex;"><span>    index
</span></span><span style="display:flex;"><span>})
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>let customQueryEngine <span style="color:#f92672">=</span> new llamaIndex<span style="color:#f92672">.</span>RetrieverQueryEngine(
</span></span><span style="display:flex;"><span>    customRetriever,
</span></span><span style="display:flex;"><span>    customSynthesizer
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>let response2 <span style="color:#f92672">=</span> <span style="color:#66d9ef">await</span> customQueryEngine<span style="color:#f92672">.</span>query({
</span></span><span style="display:flex;"><span>    query: <span style="color:#e6db74">&#34;What does the author think of college?&#34;</span>
</span></span><span style="display:flex;"><span>})
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>console<span style="color:#f92672">.</span>log(response2<span style="color:#f92672">.</span>toString())
</span></span></code></pre></div><h2 id="routing-queries">Routing Queries</h2>
<p>The ability of the RAG system to understand how to answer a query is a powerful feature.</p>
<p>Example:</p>
<ul>
<li>Route by user</li>
<li>Route by domain</li>
<li>Route by security and access privileges</li>
</ul>
<p>Whta Problems Does a RAG system have to Solve?</p>
<ul>
<li>
<p>Get the Data</p>
</li>
<li>
<p>Curate the Data</p>
<ul>
<li>Eliminate bad data (duplicates, outdated, errors, )</li>
</ul>
</li>
<li>
<p>Index the Data</p>
</li>
<li>
<p>Handle the query</p>
</li>
<li>
<p>Create the Retriever</p>
</li>
<li>
<p>Create the Synthesizer</p>
</li>
<li>
<p>Submit the Request</p>
</li>
</ul>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://localhost:63665/" >
    &copy;  An eclectic mix of technical content! 2024 
  </a>
    <div>
<div class="ananke-socials">
  
</div>
</div>
  </div>
</footer>

  </body>
</html>
