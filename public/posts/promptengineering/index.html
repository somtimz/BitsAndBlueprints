<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Prompt Engineering and In Context Learning | An eclectic mix of technical content!</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Status: Content: 10% Editing 10% Value: Medium
TODO: Look up arxiv paper on good/bad prompt engineering
Good prompt engineering is the simplest most accessible way to get value from AI models.
Guide to Prompting Make prompts as clear and concise as possible Clarify or remove ambiguities. Ensure the who, what, where, why and how are explicit and clearly defined Keep prompts short as feasible Be wary of the lost in the middle effect of long context windows Long prompts are both slower and more expensive than long prompts Add content to the context if it will help with the answer Use prompt completion pairs in the context to provide few shot learning examples Leverage the prompt structures Use the roles that models provide Leverage the system, userr and assistant roles Use instructions to guide the prompt completion Prefer to give instrucitons raher than ask questions Not what is the heaviest element, but&gt; Complete the sentence:The heaviest element commonly found in nature is: Your instructions should be at the front for short prompts and at the end for long prompts Use Chain of Thought or Reason Step by Step Experiment with prompts.">
    <meta name="generator" content="Hugo 0.123.7">
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    

    
      

    

    

    
      <link rel="canonical" href="http://localhost:1313/posts/promptengineering/">
    

    <meta property="og:title" content="Prompt Engineering and In Context Learning" />
<meta property="og:description" content="Status: Content: 10% Editing 10% Value: Medium
TODO: Look up arxiv paper on good/bad prompt engineering
Good prompt engineering is the simplest most accessible way to get value from AI models.
Guide to Prompting Make prompts as clear and concise as possible Clarify or remove ambiguities. Ensure the who, what, where, why and how are explicit and clearly defined Keep prompts short as feasible Be wary of the lost in the middle effect of long context windows Long prompts are both slower and more expensive than long prompts Add content to the context if it will help with the answer Use prompt completion pairs in the context to provide few shot learning examples Leverage the prompt structures Use the roles that models provide Leverage the system, userr and assistant roles Use instructions to guide the prompt completion Prefer to give instrucitons raher than ask questions Not what is the heaviest element, but&gt; Complete the sentence:The heaviest element commonly found in nature is: Your instructions should be at the front for short prompts and at the end for long prompts Use Chain of Thought or Reason Step by Step Experiment with prompts." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/posts/promptengineering/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-03-07T08:50:13-05:00" />
<meta property="article:modified_time" content="2024-03-07T08:50:13-05:00" />

<meta itemprop="name" content="Prompt Engineering and In Context Learning">
<meta itemprop="description" content="Status: Content: 10% Editing 10% Value: Medium
TODO: Look up arxiv paper on good/bad prompt engineering
Good prompt engineering is the simplest most accessible way to get value from AI models.
Guide to Prompting Make prompts as clear and concise as possible Clarify or remove ambiguities. Ensure the who, what, where, why and how are explicit and clearly defined Keep prompts short as feasible Be wary of the lost in the middle effect of long context windows Long prompts are both slower and more expensive than long prompts Add content to the context if it will help with the answer Use prompt completion pairs in the context to provide few shot learning examples Leverage the prompt structures Use the roles that models provide Leverage the system, userr and assistant roles Use instructions to guide the prompt completion Prefer to give instrucitons raher than ask questions Not what is the heaviest element, but&gt; Complete the sentence:The heaviest element commonly found in nature is: Your instructions should be at the front for short prompts and at the end for long prompts Use Chain of Thought or Reason Step by Step Experiment with prompts."><meta itemprop="datePublished" content="2024-03-07T08:50:13-05:00" />
<meta itemprop="dateModified" content="2024-03-07T08:50:13-05:00" />
<meta itemprop="wordCount" content="982">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Prompt Engineering and In Context Learning"/>
<meta name="twitter:description" content="Status: Content: 10% Editing 10% Value: Medium
TODO: Look up arxiv paper on good/bad prompt engineering
Good prompt engineering is the simplest most accessible way to get value from AI models.
Guide to Prompting Make prompts as clear and concise as possible Clarify or remove ambiguities. Ensure the who, what, where, why and how are explicit and clearly defined Keep prompts short as feasible Be wary of the lost in the middle effect of long context windows Long prompts are both slower and more expensive than long prompts Add content to the context if it will help with the answer Use prompt completion pairs in the context to provide few shot learning examples Leverage the prompt structures Use the roles that models provide Leverage the system, userr and assistant roles Use instructions to guide the prompt completion Prefer to give instrucitons raher than ask questions Not what is the heaviest element, but&gt; Complete the sentence:The heaviest element commonly found in nature is: Your instructions should be at the front for short prompts and at the end for long prompts Use Chain of Thought or Reason Step by Step Experiment with prompts."/>

	
  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        An eclectic mix of technical content!
      
    </a>
    <div class="flex-l items-center">
      

      
      
<div class="ananke-socials">
  
</div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked ttu">
          
        Posts
      </aside>
      










  <div id="sharing" class="mt3 ananke-socials">
    
  </div>


      <h1 class="f1 athelas mt3 mb1">Prompt Engineering and In Context Learning</h1>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2024-03-07T08:50:13-05:00">March 7, 2024</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>Status: Content: 10% Editing 10% Value: Medium</p>
<p>TODO: Look up arxiv paper on good/bad prompt engineering</p>
<p>Good prompt engineering is the simplest most accessible way to get value from AI models.</p>
<h2 id="guide-to-prompting">Guide to Prompting</h2>
<ul>
<li>Make prompts as clear and concise as possible
<ul>
<li>Clarify or remove ambiguities.</li>
<li>Ensure the who, what, where, why and how are explicit and clearly defined</li>
</ul>
</li>
<li>Keep prompts short as feasible
<ul>
<li>Be wary of the lost in the middle effect of long context windows</li>
<li>Long prompts are both slower and more expensive than long prompts</li>
</ul>
</li>
<li>Add content to the context if it will help with the answer</li>
<li>Use prompt completion pairs in the context to provide few shot learning examples</li>
<li>Leverage the prompt structures
<ul>
<li>Use the roles that models provide</li>
<li>Leverage the system, userr and assistant roles</li>
<li>Use instructions to guide the prompt completion</li>
</ul>
</li>
<li>Prefer to give instrucitons raher than ask questions
<ul>
<li>Not what is the heaviest element, but&gt;</li>
</ul>
<pre tabindex="0"><code>Complete the sentence:
The heaviest element commonly found in nature is:
</code></pre></li>
<li>Your instructions should be at the front for short prompts and at the end for long prompts</li>
<li>Use Chain of Thought or Reason Step by Step</li>
<li>Experiment with prompts.
<ul>
<li>The first prompt may not be the best</li>
<li>Understand the subtleties of prompting for each model</li>
</ul>
</li>
</ul>
<p>Leverage model paramters</p>
<ul>
<li>Temperature of 1 means the model probabilities output is that learned during training
<ul>
<li>&lt;1 means probabilities are compressed - can result in more repetition/predcitability</li>
<li>
<blockquote>
<p>1 means proababilities are dispersed - less repotation, more creaive ioputpu, but if too high can become nonsensical</p>
</blockquote>
</li>
<li>In general, use a low temperature for more predictable outcomes and a high temperature to encourage creativity</li>
</ul>
</li>
<li>Set max new tokens to control howe many tokens are generated - a small # will limit how long an answer will be</li>
<li>Top p seampling (nucleus sampling). If you use Top P it means that only the tokens comprising the top_p probability mass are considered for responses, so a low top_p value selects the most confident responses.
<ul>
<li>A high top_p value will enable the model to look at more possible words, including less likely ones, leading to more diverse outputs.</li>
<li>The general recommendation is to alter temperature or Top P but not both.</li>
</ul>
</li>
<li>A stop sequence is a string that stops the model from generating tokens.
<ul>
<li>Specifying stop sequences is another way to control the length and structure of the model&rsquo;s response. For example, you can tell the model to generate lists that have no more than 10 items by adding &ldquo;11&rdquo; as a stop sequence.</li>
</ul>
</li>
<li>Frequency Penalty - The frequency penalty applies a penalty on the next token proportional to how many times that token already appeared in the response and prompt. The higher the frequency penalty, the less likely a word will appear again. This setting reduces the repetition of words in the model&rsquo;s response by giving tokens that appear more a higher penalty.</li>
<li>Presence Penalty - The presence penalty also applies a penalty on repeated tokens but, unlike the frequency penalty, the penalty is the same for all repeated tokens. A token that appears twice and a token that appears 10 times are penalized the same. This setting prevents the model from repeating phrases too often in its response. If you want the model to generate diverse or creative text, you might want to use a higher presence penalty. Or, if you need the model to stay focused, try using a lower presence penalty.</li>
<li>Similar to temperature and top_p, the general recommendation is to alter the frequency or presence penalty but not both.</li>
</ul>
<h2 id="examples">Examples</h2>
<ul>
<li></li>
</ul>
<h2 id="subleties">Subleties</h2>
<ul>
<li>
<p>Remember that after every pass a model will produce a probability for every word in the vocabulary</p>
</li>
<li>
<p>How do top-p and top-k impact each other?</p>
</li>
<li>
<p>TLDR:</p>
<ul>
<li>Use well worded prompts</li>
<li>Keep context windows, short, concise and relevant</li>
<li>Leverage prompt structures</li>
<li>Use of few shot learning
<ul>
<li>Use of prompt/completion pairs is a useful technique</li>
</ul>
</li>
<li>Use of step by step reasoning</li>
<li>Consult the model card for a model for the best prompts to use</li>
</ul>
</li>
</ul>
<h2 id="what">What</h2>
<h2 id="why">Why</h2>
<h2 id="how">How</h2>
<ul>
<li>Use input and optionally output indicators to guide the prompt completion
<ul>
<li>&lsquo;User:&rsquo; is often referred to as the input indicator</li>
<li>&lsquo;Assistant:&rsquo; is often referred to as the output indicator.</li>
</ul>
</li>
<li></li>
</ul>
<h3 id="prompt-engineering-strategiesd-for-different-models">Prompt Engineering Strategiesd for Different Models</h3>
<p>Prompt structures are very model-specific. Using different input and output indicators may result in “off-distribution” and undesirable results. Always look up the prompt structure when you start experimenting with a new generative AI model. The information can oftne be found in model documentation such as the model card.</p>
<p>TODO: Look up the model cards for sopme models</p>
<h4 id="openai-models">OpenAI Models</h4>
<h2 id="advanced-techniques">Advanced Techniques</h2>
<ul>
<li>Use inference configurationon parameters to control the behavior of the AI model
<ul>
<li>max new tokens - a small # will limit how long an answer will be</li>
<li>sample top k</li>
<li>sample top p</li>
<li>Temperature</li>
</ul>
</li>
</ul>
<h2 id="subleties-1">Subleties</h2>
<ul>
<li>Remember that after every pass a model will produce a probability for every word in the vocabulary</li>
<li>How do top-p and top-k impact each other?</li>
<li>Temperature of 1 means the model probabilities output is that learned during training
<ul>
<li>&lt;1 means probabilities are compressed - can result in more repetition/predcitability</li>
<li>
<blockquote>
<p>1 means proababilities are dispersed - less repotation, more creaive ioputpu, but if too high can become nonsensical</p>
</blockquote>
</li>
<li>Advise: In general, use a low temperature for more predictable outcomes and a high temperature to encourage creativity</li>
</ul>
</li>
</ul>
<p>FUTURE: What other docidng stratgies are there</p>
<h2 id="skill-testing-questions">Skill Testing Questions</h2>
<p>What prompt techniques reduce hallucinations?
Do inference parmaeters impact hallucinations?
Explain and contrast the difference between greedy sampling and random sampling
What parameters influnce random sampling behavior?
Explain how top-k and top-p impact how (random)sampling works
What is the difference in outcomes between a low termperature and a high temperature?
- what is a high temperature?</p>
<h2 id="thinking-out-loud">Thinking Out Loud</h2>
<ul>
<li>Suppose I want to select 3-5 phrases rather than the next word
<ul>
<li>Approach 1:
<ul>
<li>I keep generating words at random until i have a phrase</li>
<li>Q: Can i generate a phrase? How exactly?</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://localhost:1313/" >
    &copy;  An eclectic mix of technical content! 2024 
  </a>
    <div>
<div class="ananke-socials">
  
</div>
</div>
  </div>
</footer>

  </body>
</html>
